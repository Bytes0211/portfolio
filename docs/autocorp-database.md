# AutoCorp Cloud Data Lake Pipeline

**Last Updated:** December 29, 2025  
**Project Status:** Phase 4 Complete ‚úÖ | Phase 5 AI Chatbox In Progress (20%)  
**Duration:** 6 weeks (Nov 18 - Jan 10, 2026)

An end-to-end **AWS Data Lake Pipeline** implementing modern data lakehouse architecture with CDC replication, serverless ETL, and open table formats. This project demonstrates enterprise-scale data engineering capabilities beyond traditional database management.

## üéØ Project Overview

AutoCorp is a comprehensive cloud-based data platform that extends from operational database to data lake, data warehouse, and analytics. The system implements AWS-native data lakehouse architecture enabling scalable analytics, real-time querying, and business intelligence across auto parts sales and service data.

**Tech Stack**: AWS (DMS, DataSync, Glue, S3, Athena, Bedrock Nova Pro), Apache Hudi, Terraform, PostgreSQL, PySpark, Python, Next.js  
**Architecture**: 3-layer data lakehouse (Raw ‚Üí Processed ‚Üí Curated) + AI Layer (RAG with Bedrock)  
**Data Volume**: 1.6M operational records (PostgreSQL), 792K unique orders in CSV (7.27M rows), 1.19M total unique orders
**Data Latency**: <15 minutes end-to-end (source to queryable)  
**Infrastructure as Code**: Terraform with 95% automation (8 modules, 50+ resources defined)
**ETL Jobs**: 10 total (7 operational + 3 analytics denormalized tables)
**Data Quality Testing**: Intentional CSV duplicates demonstrate Hudi upsert deduplication (invoice_number as record key)
**Project Status**: Phase 4 Complete ‚úÖ | Phase 5 AI Chatbox In Progress (20% - Bedrock Infrastructure Ready)

## üèóÔ∏è Infrastructure as Code Implementation

The entire AWS infrastructure is deployed using **Terraform with 95% automation coverage**, demonstrating modern Infrastructure as Code practices:

**Terraform Structure:**
- **8 Reusable Modules**: S3, IAM, Secrets Manager, Glue, Athena, Monitoring, DMS, Bedrock
- **Multi-Environment Support**: dev/staging/prod configurations  
- **Remote State Management**: S3 + DynamoDB state locking
- **Infrastructure Resources**: 50+ total (deployed across Phases 1-4)
- **Code Metrics**: ~2,500+ lines of Terraform + PySpark across all modules
- **Cost-Optimized**: S3 lifecycle policies, right-sized instances (~$50-80/month with analytics layer)

**Module Status (as of December 29, 2025):**
- ‚úÖ S3 Module (DEPLOYED): Data lake buckets, lifecycle policies, encryption
- ‚úÖ IAM Module (DEPLOYED): Service roles with least privilege
- ‚úÖ Secrets Module (DEPLOYED): Secure PostgreSQL credential storage
- ‚úÖ Glue Module (DEPLOYED): Catalog, 3 crawlers, 10 ETL jobs (7 operational + 3 analytics)
- ‚úÖ Athena Module (DEPLOYED): Workgroup with 5 named queries, result configuration
- ‚úÖ Monitoring Module (DEPLOYED): CloudWatch dashboard with 8 widgets, 3 alarms (Glue, Athena, Cost)
- ‚úÖ DMS Module (IaC COMPLETE): 361 lines, ready for deployment (execution deferred)
  - Replication instance (dms.t3.medium)
  - Source endpoint (PostgreSQL with Secrets Manager integration)
  - Target endpoint (S3 Parquet with SNAPPY compression)
  - Table mappings (7 tables with pg_ prefix transformation)
  - Full load + CDC tasks defined
- ‚öôÔ∏è Bedrock Module (IN PROGRESS): OpenSearch Serverless, Knowledge Base with RAG, Titan Embeddings
  - 1,584 knowledge base documents uploaded to S3
  - Terraform module complete (240 lines)
- üìù DataSync Module (DOCUMENTED): Production deployment guide (11KB), S3 CLI for dev

**Comprehensive Documentation:**
- **6,500+ lines** of technical documentation (20+ files)
- Developer's Journal - Updated through Phase 4 (2,515 lines)
- Developer approach with complete architecture (890+ lines)
- IaC Feasibility Assessment (588 lines)
- Phase 1 Deployment Complete (495 lines)
- Operations Runbook (614 lines - comprehensive operational guide)
- Interview Questions & Answers (2,430 lines - comprehensive prep guide)
- Data Quality Testing guide (665 lines - merged and enhanced)
- Project Status with Gantt Chart (585 lines - through Phase 5)
- Phase 5 AI Chatbox Implementation (760 lines)
- Terraform deployment guide (297 lines)
- Data Quality Quick Reference (136 lines)
- Process Flow Diagrams (596 lines - 10 Mermaid diagrams)
- AWS Sanity Checks guide (594 lines)
- Cost Management guide (486 lines)

## ‚òÅÔ∏è Cloud Data Platform Architecture

### Modern Data Lakehouse Implementation

This project showcases a production-grade AWS data platform with the following layers:

**1. Ingestion Layer**
- **AWS DMS**: Continuous database extraction with Change Data Capture (CDC) from PostgreSQL
- **AWS DataSync**: High-performance transfer of large CSV files (multi-GB) from on-premises to cloud

**2. Raw Storage Layer (Data Lake)**
- **AWS S3**: Scalable object storage with partitioned structure (date-based)
- Landing zone for all raw data in Parquet format before transformation

**3. Catalog & Metadata Layer**
- **AWS Glue Data Catalog**: Centralized metadata repository (Hive Metastore compatible)
- **AWS Glue Crawlers**: Automated schema discovery and catalog synchronization

**4. Processing Layer (ETL)**
- **AWS Glue ETL Jobs**: Serverless PySpark data cleaning, transformation, quality validation
- Data deduplication, standardization, and business rule application

**5. Curated Layer (Data Warehouse)**
- **Apache Hudi Tables**: Open table format providing ACID transactions, upserts, and time-travel
- Analytics-ready datasets optimized for query performance

**6. Query & Analytics Layer**
- **AWS Athena**: Serverless SQL query engine directly on S3 data
- No data movement required, pay-per-query model
- Compatible with BI tools (Tableau, PowerBI, QuickSight)

### Architecture Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Source Systems      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ PostgreSQL DB       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ AWS DMS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ - 7 tables          ‚îÇ      (CDC Replication)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                           ‚îÇ
                                                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚ñº
‚îÇ CSV Files           ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îÇ   S3 Data Lake         ‚îÇ
‚îÇ customers.csv       ‚îÇ‚îÄ‚îÄ‚ñ∫ DataSync ‚îÄ‚ñ∫   (Raw Zone)           ‚îÇ
‚îÇ sales_orders.csv    ‚îÇ              ‚îÇ - /raw/database/       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ - /raw/csv/            ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ   AWS Glue             ‚îÇ
                                     ‚îÇ - Crawler (catalog)    ‚îÇ
                                     ‚îÇ - ETL Jobs (clean)     ‚îÇ
                                     ‚îÇ - Data Quality Rules   ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ   S3 Data Lake         ‚îÇ
                                     ‚îÇ   (Curated Zone)       ‚îÇ
                                     ‚îÇ - Apache Hudi tables   ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ   AWS Athena           ‚îÇ
                                     ‚îÇ   (Query Engine)       ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                  ‚îÇ
                                                  ‚ñº
                                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     ‚îÇ   BI Tools             ‚îÇ
                                     ‚îÇ - Tableau/PowerBI     ‚îÇ
                                     ‚îÇ - QuickSight          ‚îÇ
                                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Phase 5 Extension (Planned - AI Layer):**

The architecture will extend with an AI-powered interface:
```
[Existing Architecture] ‚Üí Amazon Bedrock Nova Pro (RAG) ‚Üí Lambda + API Gateway ‚Üí Next.js Chatbox
                         ‚Üë
                   Knowledge Base
              (OpenSearch Serverless)
```

Enabling:
- Natural language queries ("What parts are needed for an oil change?")
- Real-time analytics ("Top-selling parts this month?")
- Customer support automation with context from 1.19M orders, 400+ parts, 110 services

## üöÄ Key Technical Achievements

### CDC Replication & Real-Time Data
- **Change Data Capture**: PostgreSQL changes replicated to S3 with <5 minute lag
- **Continuous Sync**: DMS tasks maintain real-time data synchronization
- **End-to-End Latency**: <15 minutes from source database to queryable analytics layer

### Open Table Formats (Apache Hudi)
- **ACID Transactions**: Hudi provides atomicity, consistency, isolation, durability on S3
- **Upsert Operations**: Efficient merge-on-read for incremental updates
- **Time-Travel Queries**: Query historical data as of specific timestamps
- **Incremental Processing**: Process only changed records, not full table scans

### Serverless ETL with PySpark

**Sample Glue ETL Job for Hudi:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Initialize Spark with Hudi
spark = SparkSession.builder \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .getOrCreate()

# Read from raw zone
df = spark.read.parquet("s3://autocorp-datalake/raw/database/sales_order/")

# Data quality checks
df_clean = df \
    .dropDuplicates(["order_id"]) \
    .filter(col("total_amount") > 0) \
    .withColumn("etl_timestamp", current_timestamp())

# Write to Hudi with upsert
hudi_options = {
    'hoodie.table.name': 'sales_order',
    'hoodie.datasource.write.recordkey.field': 'order_id',
    'hoodie.datasource.write.partitionpath.field': 'order_date',
    'hoodie.datasource.write.operation': 'upsert',
    'hoodie.datasource.write.precombine.field': 'updated_at',
    'hoodie.upsert.shuffle.parallelism': 20
}

df_clean.write \
    .format("hudi") \
    .options(**hudi_options) \
    .mode("append") \
    .save("s3://autocorp-datalake/curated/hudi/sales_order/")
```

### Large-Scale Data Handling
- **PostgreSQL Data**: 397,146 orders (1.6M total rows) for DMS CDC testing
- **CSV Processing**: 791,532 unique orders (1.86M rows with intentional duplicates, 7.27M total)
- **Partitioning Strategy**: Date-based partitions for optimal query performance
- **Compression**: Parquet format with SNAPPY compression for storage efficiency
- **Scalability**: Architecture handles multi-GB file sizes and 1.19M unique orders
- **Data Quality**: CSV duplicates demonstrate Hudi deduplication capabilities

### Data Quality Testing Framework

Real-world data quality issue simulation demonstrating ETL deduplication capabilities:

**Duplicate Record Handling:**
- **CSV files contain intentional duplicates**: 791,532 unique orders appearing as 1,864,774 total rows
- **Simulates real-world scenario**: Source systems often have duplicate records from multiple extracts
- **Demonstrates Hudi upserts**: invoice_number used as record key for automatic deduplication
- **Skills showcase**: Handling messy data at scale using Apache Hudi's ACID capabilities

**Implementation Strategy:**
```python
# Hudi configuration for deduplication
hudi_options = {
    'hoodie.table.name': 'sales_order',
    'hoodie.datasource.write.recordkey.field': 'invoice_number',  # Primary key
    'hoodie.datasource.write.precombine.field': 'order_date',     # For conflict resolution
    'hoodie.datasource.write.operation': 'upsert',                # Automatic deduplication
}

# Hudi automatically handles duplicates during write
df.write.format("hudi").options(**hudi_options).save(output_path)
```

**Why This Approach:**
- **Real-world relevance**: Production pipelines frequently encounter duplicates
- **Defensive programming**: Shows understanding of data quality issues
- **Hudi value demonstration**: Highlights why Hudi was chosen over plain Parquet
- **Interview talking point**: Concrete example of handling data quality at scale

**Data Volume:**
- PostgreSQL: 397,146 orders (for DMS CDC testing)
- CSV files: 791,532 unique orders, 1,864,774 total rows (2.35x duplication rate)
- Combined: 1,188,678 total unique orders across both sources

## üóÑÔ∏è Source Database System (PostgreSQL)

The pipeline begins with a production PostgreSQL database that serves as the operational data source:

**Database**: `autocorp`  
**Total Tables**: 7  
**Data Volume**: 1,605,804 records across all tables (PostgreSQL operational database)

## üèóÔ∏è Database Architecture

### Schema Design

The system implements a normalized relational design with the following core components:

#### **Parts & Services**
- `auto_parts` (400 parts) - Complete parts inventory with SKU, description, category, pricing
- `service` (110 services) - Service catalog spanning 11 categories with labor rates and durations
- `service_parts` (1,074 mappings) - Many-to-many relationship tracking parts required for each service

#### **Customer Management**
- `customers` (1,149 customers) - Customer information with contact details and geographic distribution

#### **Sales System**
- `sales_order` - Order headers with customer reference, payment method, totals, and order type
- `sales_order_parts` - Line items for parts sales
- `sales_order_services` - Line items for service sales

### Key Features

**Unified Sales Model**
- Single invoice can contain parts, services, or both
- Three order types: `Parts`, `Service`, `Mixed`
- Automatic order type classification based on line item composition

**Service-Parts Integration**
- Each service linked to required parts via junction table
- Quantity tracking for part-service relationships
- Average of 9.8 parts per service
- Complex service example: Service ID `92038482` requires 27 different parts

**Data Integrity**
- Foreign key constraints throughout
- Email uniqueness in customer table
- Referential integrity between orders and customers
- Proper normalization with no data redundancy

## üí° Technical Highlights

### Database Design Patterns

**Junction Table Implementation**
```sql
service_parts
‚îú‚îÄ‚îÄ serviceid (FK ‚Üí service)
‚îú‚îÄ‚îÄ sku (FK ‚Üí auto_parts)
‚îî‚îÄ‚îÄ quantity (int)
```
Enables many-to-many relationships between services and parts with quantity metadata.

**Flexible Order System**
```sql
sales_order
‚îú‚îÄ‚îÄ order_id (PK)
‚îú‚îÄ‚îÄ customer_id (FK ‚Üí customers)
‚îú‚îÄ‚îÄ order_type (Parts|Service|Mixed)
‚îú‚îÄ‚îÄ payment_method
‚îî‚îÄ‚îÄ total_amount

sales_order_parts (0..*)     sales_order_services (0..*)
‚îú‚îÄ‚îÄ order_id (FK)            ‚îú‚îÄ‚îÄ order_id (FK)
‚îú‚îÄ‚îÄ sku (FK)                 ‚îú‚îÄ‚îÄ serviceid (FK)
‚îú‚îÄ‚îÄ quantity                 ‚îú‚îÄ‚îÄ labor_minutes
‚îî‚îÄ‚îÄ line_total               ‚îî‚îÄ‚îÄ line_total
```

### SQL Capabilities Demonstrated

**Complex Joins**
- Multi-table joins across services, parts, and orders
- Junction table navigation
- Aggregations with GROUP BY and HAVING clauses

**Data Analysis Queries**
- Service complexity analysis (parts per service)
- Geographic customer distribution
- Order analytics and reporting
- Inventory management queries

**Transactions**
- Multi-table inserts for complete orders
- ACID compliance for order creation
- Rollback capabilities for data integrity

## üìä Service Categories

The system includes 110 services across 11 automotive categories:

- Engine & Powertrain
- Transmission & Drivetrain  
- Tires & Wheels
- Brakes
- Cooling System
- Electrical System
- HVAC
- Suspension & Steering
- General Preventive Maintenance
- Exhaust & Emissions
- Fluids & Filters

## üîß Implementation Details

### Data Loading Strategy

**Customer Data Sampling**
- Source: 1.2M customer records in CSV
- Loaded: 1,149 customers via random sampling
- Python script handles duplicate detection and progress reporting
- Geographic diversity: 59 different states

**Parts & Services**
- 400 parts with complete SKU, category, and pricing information
- 110 services with labor rates and duration estimates
- 1,074 service-to-parts mappings establishing part requirements

### Python Integration

**`upload_customers.py`** - Production-ready data loader
- Random sampling from large datasets
- Duplicate email handling
- Progress reporting
- Error handling and rollback
- Connection pooling

### SQL Scripts

- `create_auto_parts_table.sql` - Parts table schema
- `create_service_table.sql` - Service catalog schema  
- `create_sales_system.sql` - Complete order system with views

## üìà Example Queries

### Get Service with Required Parts
```sql
SELECT 
    s.service,
    s.category,
    s.labor_cost,
    sp.sku,
    sp.quantity,
    ap.description,
    ap.price
FROM service s
JOIN service_parts sp ON s.serviceid = sp.serviceid
JOIN auto_parts ap ON sp.sku = ap.sku
WHERE s.serviceid = '48392017';
```

### Analyze Service Complexity
```sql
SELECT 
    s.serviceid,
    s.service,
    s.category,
    COUNT(sp.sku) as parts_count,
    SUM(sp.quantity) as total_parts_needed
FROM service s
LEFT JOIN service_parts sp ON s.serviceid = sp.serviceid
GROUP BY s.serviceid, s.service, s.category
ORDER BY parts_count DESC
LIMIT 10;
```

### Create Complete Order
```sql
-- Insert order header
INSERT INTO sales_order (customer_id, order_date, invoice_number, 
                         payment_method, total_amount, order_type)
VALUES (1, CURRENT_TIMESTAMP, 'INV-2025-001', 'Credit Card', 
        145.80, 'Service')
RETURNING order_id;

-- Insert service line item
INSERT INTO sales_order_services (order_id, serviceid, labor_minutes, 
                                   labor_cost, parts_cost, line_total)
VALUES (1, '48392017', 30, 45.00, 90.00, 135.00);
```

## üì¶ Project Files

### Comprehensive Documentation (3,103+ lines)

**Architecture & Design:**
- `developer-approach.md` (854 lines) - Complete technical architecture
- `IAC_FEASIBILITY_ASSESSMENT.md` (588 lines) - Terraform feasibility analysis
- `PROJECT_GANTT_CHART.md` (307 lines) - 4-week timeline with Gantt chart
- `README.md` (479 lines) - Cloud data platform overview

**Infrastructure:**
- `terraform/README.md` (297 lines) - Deployment guide
- `terraform/` directory - 6 modules, 25 files, complete IaC structure

**Database & SQL:**
- `DATABASE_STATUS.md` - Complete schema and statistics
- `SALES_SYSTEM_USAGE.md` - Usage guide with 10+ query examples

### Source Data (CSV)
- `auto-parts.csv` - Parts inventory (400 records)
- `auto-service.csv` - Service catalog (110 records)
- `service-parts.csv` - Service-parts mappings (1,074 records)
- `customers.csv` - Customer data source (1.2M records)

### Database Scripts (SQL)
- Table creation scripts for all entities
- Complete sales system implementation
- View definitions for reporting

### Python Scripts
- `upload_customers.py` - Random customer data loader with progress reporting
- `generate_sales_orders.py` - Production-scale data generator
  - PostgreSQL: 397,146 orders for DMS CDC testing
  - CSV: 791,532 unique orders (1.86M rows with intentional duplicates)
  - Demonstrates hybrid data sources (streaming + batch)
  - Data quality feature: CSV duplicates for Hudi deduplication testing

## üöÄ Setup & Usage

### Prerequisites
- PostgreSQL installed and running
- Python 3.12+ with venv support
- Database `autocorp` created

### Quick Start

```bash
# 1. Create virtual environment
python3 -m venv .venv
.venv/bin/pip install -r requirements.txt

# 2. Create database tables
psql -U scotton -d autocorp -f create_auto_parts_table.sql
psql -U scotton -d autocorp -f create_service_table.sql
psql -U scotton -d autocorp -f create_sales_system.sql

# 3. Load data
.venv/bin/python upload_customers.py
```

### Database Status Check

```bash
psql -U scotton -d autocorp -c "
  SELECT 'auto_parts' as table_name, COUNT(*) FROM auto_parts 
  UNION ALL SELECT 'customers', COUNT(*) FROM customers 
  UNION ALL SELECT 'service', COUNT(*) FROM service 
  UNION ALL SELECT 'service_parts', COUNT(*) FROM service_parts
  ORDER BY table_name;"
```

## üíº Skills Demonstrated

**Cloud Data Engineering**
- AWS data lakehouse architecture design and implementation
- CDC (Change Data Capture) replication with AWS DMS
- Serverless ETL with AWS Glue and PySpark
- Open table formats (Apache Hudi) for ACID transactions on data lakes
- Data lake design patterns (Raw ‚Üí Processed ‚Üí Curated)

**Infrastructure as Code (Terraform)**
- Complete AWS infrastructure automation (95% coverage)
- Multi-environment deployment (dev/staging/prod)
- Remote state management with S3 + DynamoDB locking
- Modular design with 6 reusable Terraform modules
- Cost optimization and security best practices

**AWS Services**
- **AWS DMS**: Database migration and continuous replication
- **AWS DataSync**: High-performance file transfer
- **AWS Glue**: Data catalog, crawlers, ETL jobs
- **AWS S3**: Data lake storage with lifecycle policies
- **AWS Athena**: Serverless SQL query engine

**Data Processing**
- PySpark for distributed data processing
- Data quality validation and deduplication
- Incremental processing with Hudi upserts
- Large-scale CSV file handling (multi-GB)
- Partitioning strategies for query optimization

**Data Quality & Testing**
- Real-world duplicate record handling with Hudi upserts
- CSV duplicates (2.35x rate) demonstrate ETL deduplication at scale
- invoice_number as Hudi record key for automatic duplicate removal
- Simulates production data quality issues
- Showcases Apache Hudi's ACID capabilities on data lakes

**Database Design**
- Relational schema design and normalization
- Junction tables for many-to-many relationships
- Foreign key constraints and referential integrity
- Index optimization for query performance

**SQL Proficiency**
- Complex multi-table joins
- Aggregate functions and grouping
- Subqueries and CTEs
- Transaction management
- Athena/Presto SQL for data lake queries

**Python & SQL Integration**
- psycopg2 for database connectivity
- Parameterized queries for security
- Error handling and rollbacks
- Progress reporting and logging

**Documentation & Project Management**
- Comprehensive technical documentation (4,670+ lines across 13 files)
- Architecture decision records (ADRs)
- Project timeline with Gantt charts
- Risk assessment and mitigation strategies
- Implementation roadmap with 4-week plan
- Data quality testing documentation (462 lines)

## üìä Data Statistics

**Operational Database (PostgreSQL):**
- **Total records**: 1,605,804 across all tables
- **Sales orders**: 397,146 orders
- **Parts line items**: 853,591 rows
- **Service line items**: 355,067 rows
- **Parts catalog**: 400 parts
- **Service catalog**: 110 services across 11 categories
- **Service-parts mappings**: 1,074 relationships
- **Customer base**: 1,149 active customers
- **Average parts per service**: 9.8 parts
- **Most complex service**: 27 parts required
- **Geographic distribution**: 59 states represented

**Documentation:**
- **Total documentation**: 4,670+ lines across 13 files
- **Technical depth**: 890+ line developer approach document
- **IaC coverage**: 95% automation with Terraform
- **Project planning**: 4-week Gantt chart with 380 lines
- **Data quality testing**: 462 lines across 2 comprehensive guides
- **Implementation journal**: 911 lines documenting Phase 2

**CSV Test Data (For DataSync):**
- **Unique orders**: 791,532 unique orders
- **Total rows with duplicates**: 1,864,774 rows (sales_orders.csv)
- **Parts line items**: 4,877,041 rows
- **Service line items**: 529,488 rows
- **Total CSV data**: 7,271,303 rows across 3 files
- **Duplication rate**: 2.35x (intentional for ETL testing)
- **Data quality feature**: Demonstrates Hudi upsert deduplication
- **Combined dataset**: 1,188,678 total unique orders (PostgreSQL + CSV)

## üéØ Project Status

**Current Phase**: Phase 3 Complete (IaC) | Phase 4 Analytics In Progress (80% Overall) ‚úÖ

**Phase 1 - Infrastructure Foundation (Nov 22, 2025)** ‚úÖ 100%:
- ‚úÖ PostgreSQL source database with 7 tables (5,668 records operational)
- ‚úÖ Complete Terraform IaC deployment (35 AWS resources)
  - S3 data lake with lifecycle policies
  - IAM roles (Glue, DMS, DataSync)
  - Secrets Manager for credentials
  - Glue Data Catalog database
  - 2 Glue Crawlers (raw-database, raw-csv)
- ‚úÖ Remote state management (S3 + DynamoDB)
- ‚úÖ Complete infrastructure documentation (495 lines)

**Phase 2 - Glue ETL with Apache Hudi (Dec 7, 2025)** ‚úÖ 100%:
- ‚úÖ **7 Production PySpark ETL scripts** (535 lines total)
  - sales_order, customers, auto_parts, service
  - service_parts, sales_order_parts, sales_order_services
- ‚úÖ **7 AWS Glue jobs deployed** via Terraform
  - Glue 4.0 with Apache Hudi connector
  - G.1X workers (2 per job)
  - Job bookmarking and CloudWatch logging enabled
- ‚úÖ **All 7 Hudi tables tested and validated**
  - auto_parts (400 records)
  - customers (1,149 records)
  - service (110 records)
  - service_parts (1,074 records)
  - 3 sales tables ready for 1M orders
- ‚úÖ **2 Glue Crawlers operational** (raw-database, raw-csv)
- ‚úÖ **35+ Data quality validation rules** implemented across all ETL jobs
- ‚úÖ **End-to-end pipeline testing complete** (<15 minute latency validated)
- ‚úÖ **Test data processed** (2,733 records via Hudi)
- ‚úÖ **Developer's Journal** (911 lines documenting Phase 2 implementation)

**Phase 2.5 - Data Preparation (Dec 7, 2025)** ‚úÖ 100%:
- ‚úÖ **Generated 1.19M unique orders** for Phase 3 testing
  - PostgreSQL: 397,146 orders (1.6M total rows) for DMS CDC testing
  - CSV files: 791,532 unique orders (1.86M rows with intentional duplicates, 7.27M total)
- ‚úÖ Demonstrates **hybrid architecture** (streaming + batch ingestion)
- ‚úÖ **Data quality testing feature**: CSV duplicates showcase Hudi deduplication
  - invoice_number as record key for automatic duplicate removal
  - Simulates real-world data issues (2.35x duplication rate)
- ‚úÖ All data validated and ready for Phase 3 deployment

**Phase 3 - DMS Infrastructure as Code (Dec 7-16, 2025)** ‚úÖ 100% (IaC):
- ‚úÖ PostgreSQL logical replication configured (wal_level=logical)
- ‚úÖ **DMS Terraform module complete** (361 lines across 3 files)
  - Replication instance: dms.t3.medium with 50GB storage
  - Source endpoint: PostgreSQL with Secrets Manager integration
  - Target endpoint: S3 with Parquet format and SNAPPY compression
  - Table mappings: 7 tables with pg_ prefix transformation
  - Full load + CDC tasks defined
- ‚úÖ **DataSync documented** (11KB production deployment guide)
  - Comprehensive agent setup and configuration
  - Hybrid approach: S3 CLI for dev, DataSync for production
- ‚úÖ Ready for one-command deployment: `terraform apply -target=module.dms`
- üìù **Deployment decision**: IaC complete, actual deployment deferred (cost optimization: $125/month)

**Phase 4 - Analytics & Query Layer (Dec 16-20, 2025)** üü° 20%:
- ‚úÖ **3 Analytics ETL scripts created** (435 lines PySpark)
  - analytics_sales_order_fact_etl.py (145 lines) - Sales fact table
  - analytics_sales_order_line_items_etl.py (162 lines) - Denormalized line items
  - analytics_service_parts_catalog_etl.py (128 lines) - Service catalog view
- ‚è∏Ô∏è Configure Athena workgroups and table definitions (in progress)
- ‚è∏Ô∏è Test queries on Hudi tables
- ‚è∏Ô∏è Optimize query performance (<30s target)
- ‚è∏Ô∏è Test time-travel and incremental queries
- ‚è∏Ô∏è Finalize documentation and create CloudWatch dashboards

**Phase 5 - AI Chatbox with Amazon Bedrock (Future)** üìù Planned:
- üìù **AI-powered chatbox** using Amazon Bedrock Nova Pro
- üìù **RAG Implementation** with Bedrock Knowledge Bases
  - Vector embeddings with Titan Embeddings G1
  - OpenSearch Serverless for vector storage
  - Knowledge base: 400+ parts, 110 services, 1,074 service-parts mappings
- üìù **Next.js Frontend** with TypeScript + Tailwind CSS
  - Deployed on AWS Amplify
  - Real-time chat interface with shadcn/ui components
- üìù **Lambda Functions** for chat processing and analytics queries
- üìù **API Gateway** REST endpoints for chatbox integration
- üìù **Use Cases:**
  - Customer support ("What parts are needed for an oil change?")
  - Data analytics ("What are our top-selling parts this month?")
  - Natural language interface to data lake
- üìù **Documentation ready**: 760-line implementation guide (PHASE5_AI_CHATBOX.md)
- üìù **Cost estimate**: ~$150-180/month (Bedrock + OpenSearch Serverless)

**Progress**: 80% overall (16 of 20 days complete)  
**Target Completion**: Phase 4 - December 20, 2025 | Phase 5 - Future Enhancement  
**Status**: On Track ‚úÖ

**Key Achievements:**
- üèÜ **1,800+ lines** of Terraform + PySpark code written
- üèÜ **44 AWS resources** defined (35 deployed + 9 DMS pending)
- üèÜ **10 ETL jobs** implemented (7 operational + 3 analytics)
- üèÜ **5,200+ lines** of comprehensive documentation
- üèÜ **Cost-optimized**: ~$1/month current infrastructure
- üèÜ **Roadmap complete**: Phase 5 AI chatbox fully planned (760 lines)

## üîó Repository

[View AutoCorp Project on GitHub](https://github.com/Bytes0211/autocorp)

---

**Project Type**: Cloud Data Engineering / Data Lakehouse Architecture  
**Complexity**: Enterprise-Scale  
**Status**: Architecture Complete, Implementation In Progress

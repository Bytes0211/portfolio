# StreamForge Real-Time Data Streaming Platform

**Last Updated:** December 18, 2025  
**Project Status:** Phase 1 Complete (40% overall)  
**Duration:** 2 weeks (Dec 4 - Dec 18, 2025)

A real-time data streaming and processing platform demonstrating modern stream processing architecture with Apache Kafka, Apache Flink, and cloud-native deployment patterns. This project showcases enterprise-scale event-driven architecture with dual deployment models (local and AWS production).

## ğŸ¯ Project Overview

StreamForge is a comprehensive streaming data platform that implements real-time data ingestion, transformation, and storage using industry-standard tools. The system demonstrates end-to-end stream processing capabilities with both local development infrastructure and production-ready AWS deployment.

**Tech Stack**: Apache Kafka, Apache Flink (Java), MongoDB, Docker, Maven, AWS (DynamoDB, Amplify), Terraform  
**Architecture**: Event-driven streaming with dual deployment models  
**Processing Engine**: Apache Flink 1.18.0 with Java 11  
**Messaging**: Apache Kafka 3.5.1 (Confluent Platform 7.5.0)  
**Build System**: Maven with Shade plugin for JAR packaging  
**Testing**: JUnit with embedded test harnesses  
**Project Status**: Phase 1 Complete (40% overall)

## ğŸ—ï¸ Architecture

### Dual Deployment Model

StreamForge supports two deployment architectures:

**1. Local Development Environment**
```
External Sources â†’ Kafka (localhost:9092)
                     â†“
               Flink Cluster (JobManager + TaskManager)
                     â†“
               Stream Processing (StreamProcessor.java)
                     â†“
               MongoDB (localhost:27017)
```

**2. AWS Production Environment (Planned)**
```
External Sources â†’ Kafka/Kinesis
                     â†“
               AWS Managed Flink
                     â†“
               Stream Processing
                     â†“
               DynamoDB
                     â†“
           React Frontend (AWS Amplify)
```

### Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ External Sources    â”‚
â”‚ - APIs              â”‚
â”‚ - Event Streams     â”‚
â”‚ - IoT Devices       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache Kafka      â”‚
â”‚ - Topic: streamforge-input
â”‚ - Consumer Group: streamforge-consumer-group
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Apache Flink      â”‚
â”‚ - StreamProcessor   â”‚
â”‚ - Transformations   â”‚
â”‚ - Checkpointing     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB/DynamoDB  â”‚
â”‚ - Collection: processed_data
â”‚ - Real-time storage â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Key Technical Components

### Apache Flink Stream Processing

**StreamProcessor.java** - Main entry point for stream processing:
```java
public class StreamProcessor {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // Configure Kafka source
        KafkaSource<String> source = KafkaSource.<String>builder()
            .setBootstrapServers("kafka:29092")
            .setTopics("streamforge-input")
            .setGroupId("streamforge-consumer-group")
            .setValueOnlyDeserializer(new SimpleStringSchema())
            .build();
        
        // Stream processing pipeline
        DataStream<String> stream = env.fromSource(source, 
            WatermarkStrategy.noWatermarks(), "Kafka Source");
        
        // Apply transformations
        DataStream<String> processed = stream
            .map(value -> value.toUpperCase());
        
        // Sink to MongoDB
        processed.addSink(new MongoDBSink());
        
        env.execute("StreamForge Processor");
    }
}
```

**Key Features:**
- Kafka integration with consumer group management
- Custom sink implementation for MongoDB
- Filesystem state backend with checkpointing at `/tmp/flink-checkpoints`
- Extensible transformation pipeline (map/filter/window operations)

### Custom MongoDB Sink

**MongoDBSink.java** - RichSinkFunction with connection lifecycle management:
```java
public class MongoDBSink extends RichSinkFunction<String> {
    private transient MongoClient mongoClient;
    private transient MongoCollection<Document> collection;
    
    @Override
    public void open(Configuration parameters) {
        // Initialize MongoDB connection
        mongoClient = MongoClients.create("mongodb://admin:password@mongodb:27017");
        collection = mongoClient.getDatabase("streamforge")
            .getCollection("processed_data");
    }
    
    @Override
    public void invoke(String value, Context context) {
        // Write to MongoDB
        Document doc = new Document("data", value)
            .append("timestamp", System.currentTimeMillis());
        collection.insertOne(doc);
    }
    
    @Override
    public void close() {
        if (mongoClient != null) {
            mongoClient.close();
        }
    }
}
```

**Design Patterns:**
- Connection pooling via lifecycle management (open/invoke/close)
- Transient fields to prevent serialization issues
- Timestamp tracking for data lineage

### Containerized Infrastructure

**Docker Compose** orchestrates the complete development environment:

**Services:**
- **Zookeeper**: Kafka coordination (port 2181)
- **Kafka**: Message broker (ports 9092 external, 29092 internal)
- **Flink JobManager**: Cluster coordinator (port 8081 dashboard)
- **Flink TaskManager**: Execution worker
- **MongoDB**: Document database (port 27017)

**Network Configuration:**
- Bridge network: `streamforge-network`
- Service discovery via Docker DNS
- Internal Kafka address: `kafka:29092`
- External Kafka address: `localhost:9092`

### Build System

**Maven Configuration** with Shade plugin for fat JAR packaging:
- Flink dependencies use `provided` scope (available in cluster)
- Connector dependencies bundled in JAR
- Output: `flink-jobs/target/flink-jobs-1.0-SNAPSHOT.jar`

**Key Dependencies:**
- `flink-streaming-java`: Core streaming API
- `flink-connector-kafka`: Kafka integration
- `mongodb-driver-sync`: MongoDB client (version 4.11.1)
- `flink-json` + `jackson-databind`: JSON processing

## ğŸ§ª Testing Strategy

Comprehensive unit tests for all components:

**StreamProcessorTest.java:**
- Validates Kafka source configuration
- Tests transformation logic
- Verifies sink integration

**MongoDBSinkTest.java:**
- Connection lifecycle testing
- Document insertion validation
- Error handling scenarios

**MongoDBSchemaTest.java:**
- Schema validation
- Data structure verification
- Index configuration testing

## ğŸ“Š Technical Achievements

### Real-Time Stream Processing
- **Event-Driven Architecture**: Asynchronous message processing with Kafka
- **Scalable Processing**: Flink's distributed execution model
- **Fault Tolerance**: Checkpointing ensures exactly-once processing semantics
- **State Management**: Filesystem state backend (migrating to S3/RocksDB for production)

### Dual Deployment Strategy
- **Local Development**: Complete containerized environment for rapid iteration
- **AWS Production**: Terraform IaC for managed services (Kinesis, Managed Flink, DynamoDB)
- **Migration Path**: Scripts to transition from MongoDB to DynamoDB

### Docker Orchestration
- **Multi-Service Coordination**: 5 interconnected containers
- **Network Isolation**: Custom bridge network for service communication
- **Volume Management**: Persistent storage for Kafka and MongoDB
- **Health Checks**: Container health monitoring

## ğŸ¯ Development Workflow

### 1. Local Development
```bash
# Start infrastructure
cd docker && docker-compose up -d

# Build Flink job
cd flink-jobs && mvn clean package

# Submit to Flink cluster
# Via Dashboard: http://localhost:8081
# Or CLI from container
```

### 2. Testing
```bash
# Run unit tests
cd flink-jobs && mvn test

# Compile only
mvn compile

# Clean build artifacts
mvn clean
```

### 3. Monitoring
- **Flink Dashboard**: http://localhost:8081 (job status, metrics, logs)
- **Kafka**: CLI tools or GUI (Offset Explorer, Conduktor)
- **MongoDB**: Compass or CLI (`mongosh`)

### 4. Production Deployment (Planned)
```bash
# Provision AWS infrastructure
cd terraform
terraform init
terraform plan
terraform apply

# Deploy frontend
cd frontend/streamforge-ui
amplify publish
```

## ğŸ“ Project Structure

```
streamforge/
â”œâ”€â”€ docker/                       # Container orchestration
â”‚   â””â”€â”€ docker-compose.yml       # 5 services (Kafka, Flink, MongoDB)
â”œâ”€â”€ flink-jobs/                  # Stream processing logic
â”‚   â”œâ”€â”€ pom.xml                  # Maven configuration
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main/java/com/streamforge/
â”‚   â”‚   â”‚   â”œâ”€â”€ StreamProcessor.java    # Main entry point
â”‚   â”‚   â”‚   â””â”€â”€ MongoDBSink.java        # Custom sink
â”‚   â”‚   â””â”€â”€ test/java/com/streamforge/
â”‚   â”‚       â”œâ”€â”€ StreamProcessorTest.java
â”‚   â”‚       â”œâ”€â”€ MongoDBSinkTest.java
â”‚   â”‚       â””â”€â”€ MongoDBSchemaTest.java
â”‚   â””â”€â”€ target/
â”‚       â””â”€â”€ flink-jobs-1.0-SNAPSHOT.jar  # Fat JAR output
â”œâ”€â”€ frontend/                    # React application (planned)
â”‚   â””â”€â”€ streamforge-ui/
â”œâ”€â”€ terraform/                   # AWS IaC (planned)
â”‚   â”œâ”€â”€ dynamodb.tf
â”‚   â”œâ”€â”€ amplify.tf
â”‚   â””â”€â”€ variables.tf
â”œâ”€â”€ scripts/                     # Utilities
â”‚   â””â”€â”€ mongodb-to-dynamodb/    # Migration scripts
â””â”€â”€ docs/                        # Documentation
```

## ğŸ”§ Configuration Details

### Kafka Configuration
- **Topic**: `streamforge-input`
- **Consumer Group**: `streamforge-consumer-group`
- **Bootstrap Servers**: `kafka:29092` (internal), `localhost:9092` (external)
- **Partitioning Strategy**: TBD (Phase 2)

### MongoDB Configuration
- **Connection String**: `mongodb://admin:password@mongodb:27017`
- **Database**: `streamforge`
- **Collection**: `processed_data`
- **Credentials**: admin/password (dev environment)
- **Schema Design**: Flexible document structure (Phase 2)

### Flink Configuration
- **Version**: 1.18.0
- **Java**: 11
- **State Backend**: Filesystem
- **Checkpoint Directory**: `/tmp/flink-checkpoints`
- **Parallelism**: Default (configurable per job)

## ğŸ›£ï¸ Development Roadmap

### Phase 1: Infrastructure Setup âœ… (40% Complete)
- âœ… Docker Compose environment
- âœ… Kafka cluster
- âœ… Flink cluster (JobManager + TaskManager)
- âœ… MongoDB integration
- âœ… Basic StreamProcessor implementation
- âœ… Custom MongoDBSink
- âœ… Unit tests

### Phase 2: Stream Processing Enhancement (In Progress)
- [ ] Kafka topic configuration and partitioning
- [ ] MongoDB schema design and indexing
- [ ] Advanced transformations (windowing, aggregations)
- [ ] Stateful operations (keyed state, operator state)
- [ ] Complex event processing patterns

### Phase 3: AWS Deployment (Planned)
- [ ] Terraform infrastructure modules
- [ ] DynamoDB table design
- [ ] DynamoDB sink implementation
- [ ] MongoDB to DynamoDB migration scripts
- [ ] AWS Managed Flink job deployment

### Phase 4: Frontend & Visualization (Planned)
- [ ] React chatbot UI
- [ ] AWS Amplify hosting
- [ ] Real-time data visualization
- [ ] REST API for data queries
- [ ] User authentication

## ğŸ’¡ Design Decisions

### Why Apache Flink?
- **True Stream Processing**: Event-time processing with watermarks
- **State Management**: Built-in stateful operators with checkpointing
- **Scalability**: Distributed execution across task managers
- **Exactly-Once Semantics**: Fault-tolerant processing guarantees

### Why Kafka?
- **High Throughput**: Handles millions of messages per second
- **Durability**: Message persistence with configurable retention
- **Scalability**: Horizontal scaling with partitions
- **Ecosystem**: Rich connector ecosystem for sources and sinks

### Why Docker Compose for Development?
- **Reproducibility**: Consistent environment across machines
- **Isolation**: No dependency conflicts with host system
- **Speed**: Quick startup/teardown for testing
- **Portability**: Easy sharing with team members

### Dual Deployment Strategy
- **Local**: Fast iteration with full control over infrastructure
- **AWS**: Production scalability with managed services
- **Migration Path**: Clear transition from dev to prod

## ğŸ” Learning Outcomes

### Stream Processing Concepts
- Event-driven architecture patterns
- Watermarking and event-time processing
- Stateful stream processing
- Exactly-once processing semantics
- Checkpointing and fault tolerance

### Distributed Systems
- Message broker design (Kafka)
- Distributed execution (Flink cluster)
- Container orchestration (Docker)
- Service discovery and networking

### Cloud-Native Architecture
- Infrastructure as Code (Terraform)
- Managed services vs. self-hosted
- Data migration strategies
- Cost optimization (local dev vs. production)

### Software Engineering Best Practices
- Unit testing stream processing jobs
- Build automation (Maven)
- Dependency management (provided vs. bundled)
- Documentation-driven development

## ğŸš€ Next Steps

**Immediate (Phase 2):**
1. Implement windowed aggregations (tumbling, sliding windows)
2. Add stateful operations (keyed state for session tracking)
3. Design MongoDB schema with indexing strategy
4. Configure Kafka topics with replication factor

**Mid-Term (Phase 3):**
1. Complete Terraform modules for AWS deployment
2. Implement DynamoDB sink with optimistic concurrency
3. Create migration scripts with validation
4. Set up CI/CD pipeline for Flink job deployment

**Long-Term (Phase 4):**
1. Build React chatbot UI with WebSocket updates
2. Integrate AWS Amplify for hosting and auth
3. Add real-time dashboards with data visualization
4. Implement complex event processing (CEP) patterns

## ğŸ“š Documentation

- **README.md**: Quick start and project overview
- **WARP.md**: Development guidelines and build commands
- **Docker Compose**: Service configuration and networking
- **Maven POM**: Dependency management and build configuration
- **Source Code**: Inline comments and JavaDoc

## ğŸ”— Related Projects

- **AutoCorp Cloud Data Lake**: Batch processing with AWS Glue and Hudi
- **AWS Management**: Cloud infrastructure utilities
- **DynamoDB Inventory System**: NoSQL data modeling patterns

---

**Note**: This project is in active development. Stream processing logic is currently placeholder implementation (`.toUpperCase()`) and will be replaced with domain-specific transformation logic in Phase 2.
